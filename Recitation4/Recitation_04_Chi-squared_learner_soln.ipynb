{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "<hr style=\"height: 1px;\">\n",
    "<i>This code was authored by the 8.S50x Course Team, Copyright 2021 MIT All Rights Reserved.</i>\n",
    "<hr style=\"height: 1px;\">\n",
    "<br>\n",
    "\n",
    "# RECITATION 4: Chi-squared on a histogram\n",
    "\n",
    "<br>\n",
    "<!--end-block--> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## 4.0 Overview of Learning Objectives\n",
    "\n",
    "In this recitation we will explore the following objectives:\n",
    "\n",
    "- fill in information\n",
    "\n",
    "\n",
    "<br>\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## 4.1 Warm-up Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x_i$ be the x coordinates of the data points we observe, and let $f(x_i)$ be the model we are trying to fit. We assume that the data that we observe is generated from this model with some extra error, usually do to some sort of noise.\n",
    "\n",
    "$$y_i = f(x_i) + \\epsilon $$\n",
    "\n",
    "where $\\epsilon$ is a random variable drawn from a Gaussian distribution with mean $0$. Usually, in most physics experiments, we will be able to quantify the width of the distribution by making repeated measurements and calculating the standard error, which will give us a vector of standard errors, $\\sigma_i$, which are usually plotted as errorbars like in the plot above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:purple\">>>>QUESTION</span>\n",
    "\n",
    "a) So, we assume that $y_i$ is a random variable with mean $f(x_i)$, and variance $\\sigma_i^2$. Given this, what is the probability distribution for $y_i$?\n",
    "\n",
    "b) Given the answer to a), and assuming that the probability distributions for each of the $y_i$ are independent, what is the joint probability distribution?\n",
    "\n",
    "\n",
    "<!--\n",
    "#solution\n",
    "The probability distribution is given by \n",
    "\n",
    "$$P(y_i) = \\frac{1}{\\sigma_i\\sqrt{2\\pi}} \\exp\\left( -\\frac{1}{2} \\frac{(y_i - f(x_i))^2}{\\sigma_i^2}\\right)$$\n",
    "\n",
    "If we assume each of the $y_i$ are indpendent random variables, we have the joint probabilty distribution given by\n",
    "\n",
    "$$P(y_1, y_2, ... ) = \\frac{1}{(2\\pi)^{k/2}\\prod_i\\sigma_i} \\exp\\left( -\\frac{1}{2} \\sum_i\\frac{(y_i - f(x_i))^2}{\\sigma_i^2}\\right)$$\n",
    "-->\n",
    "\n",
    "<span style=\"opacity:0.5\"><i>Click this cell to display solutions.</i></span>\n",
    "\n",
    "<br>\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## 4.2 Maximum Likelihood Estimation\n",
    "\n",
    "The previous exercise gives the probability distribution of observing the data given a particular model $f(x)$. This is known as the <b>likelihood</b> function. Note that the probability of observing the data given a particular model is *not* always the same as the probability of a particular model being true given observed data (although they are related). To begin, however, we will assume that if we find the model that maximizes the probability of the observed data, this model is close to the true model. This technique is called <b>Maximum Likelihood Estimation</b> (MLE).\n",
    "\n",
    "Usually, our model $f$ is a function not only of the indpendent variable $x$ but of various parameters representing the model, lets cal them $\\alpha_1, \\alpha_2, \\alpha_3 ... \\alpha_m $. So, we would like to find the parameters that maximize the likelihood distribution, i.e.\n",
    "\n",
    "$$\\alpha^{\\text{best}}_j = \\text{argmax}_{\\alpha_j} \\left( \\frac{1}{(2\\pi)^{k/2}\\prod_i\\sigma_i} \\exp\\left( -\\frac{1}{2} \\sum_i\\frac{(y_i - f(x_i; \\alpha_1, \\alpha_2, ...))^2}{\\sigma_i^2}\\right)\\right)$$\n",
    "\n",
    "We know that since the logarithm is a monotonically increasing function, if we maximize the logarithm of this function, we maximize the function itself. This means that\n",
    "\n",
    "$$\\alpha^{\\text{best}}_j = \\text{argmax}_{\\alpha_j} \\left( -\\frac{1}{2} \\sum_i\\frac{(y_i - f(x_i; \\alpha_1, \\alpha_2, ...))^2}{\\sigma_i^2} - \\sum_i \\log (\\sigma_i\\sqrt{2\\pi}) \\right)$$\n",
    "\n",
    "The term involving the $\\sum_i \\log (\\sigma_i\\sqrt{2\\pi})$ is a constant for the data, so we can ignore it in the maximization. \n",
    "\n",
    "$$\\alpha^{\\text{best}}_j = \\text{argmax}_{\\alpha_j} \\left( -\\frac{1}{2} \\sum_i\\frac{(y_i - f(x_i; \\alpha_1, \\alpha_2, ...))^2}{\\sigma_i^2}  \\right)$$\n",
    "\n",
    "We know that the $-\\frac{1}{2}$ in front just multiplies by a constant. So, if we want to maximize a function $-\\frac{1}{2}g(z)$ with respect to $z$, we can just minimize $g(z)$ with respect to $z$, which means that\n",
    "\n",
    "$$\\alpha^{\\text{best}}_j = \\text{argmin}_{\\alpha_j} \\left(\\sum_i\\frac{(y_i - f(x_i; \\alpha_1, \\alpha_2, ...))^2}{\\sigma_i^2}  \\right)$$\n",
    "\n",
    "Therefore, the maximum likelihood solution for the parameters can be found by minimizing the sum of squared residuals divided by the variance. This sum has a name, and it known as the $\\chi^2$. It is defined again, as\n",
    "\n",
    "$$\\chi^2(\\alpha_1, \\alpha_2 ... \\alpha_m) = \\sum_i\\frac{(y_i - f(x_i; \\alpha_1, \\alpha_2, ...))^2}{\\sigma_i^2}$$\n",
    "\n",
    "So, many fitting problems that you will encounter come down to minimizing this sum. In lecture, we had a similar sum, but without the $\\sigma_i^2$ in the denominator. If we assume that each point has the same uncertainty, than minimizing this $\\chi^2$ is the same as minimizing the sum we saw in lecture. This technique that I'm showing you is known as <b>weighted least squares</b>, as opposed to the derivation we saw in lecture that was <b>ordinary least squares</b>.\n",
    "\n",
    "In lecture, we saw how this can be solved analytically, using linear algebra or vector calculus. However, we can use the computer to numerically minimize the chi-square. The most famous of these techniques is known as gradient descent, which Phil will go over in one of the lectures, and is what the Python package lmfit uses. However, now we will delve deeper into the significance of chi-square and how we can use it to assess goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:green\">>>>RUN</span>\n",
    "\n",
    "Let's see what chi-squared tells us about data on a histogram. Run the code below to read data from a file a and plot...\n",
    "\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chisquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "pred = np.genfromtxt('chisq_model.txt', delimiter=',')    # read in file\n",
    "x = np.array([n[0] for n in pred])    # get x values\n",
    "pred_y = [n[1] for n in pred]    # get y values\n",
    "plt.xlim(-1,1)\n",
    "plt.xlabel(r'$\\cos (x)$')\n",
    "plt.plot(x,pred_y,linewidth=1,drawstyle='steps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:green\">>>>RUN</span>\n",
    "\n",
    "Next, sample errors from a normal distribution and plot the data from \"chisq_data1.txt\" with random error bars.\n",
    "\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yerr = np.random.normal(scale=2,size=len(x))+8\n",
    "def plot_error(y):\n",
    "    plt.errorbar(x,y,yerr=yerr,ecolor='k',elinewidth=1,capsize=4,linestyle='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.loadtxt('chisq_data1.txt', delimiter=',')    # read in file\n",
    "plt.xlim(-1,1)\n",
    "plt.xlabel(r'$\\cos (x)$')\n",
    "plt.plot(x,pred_y,linewidth=1,drawstyle='steps')\n",
    "plt.scatter(x,data1,c='k')\n",
    "plot_error(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:green\">>>>RUN</span>\n",
    "\n",
    "Compute and print the Chi-squared coefficient and probability.\n",
    "\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2,p = chisquare(data1,pred_y)\n",
    "print(\"Chi-squared:\",chi2)\n",
    "print(\"Probability: {:.4%}\".format(p)) #a bit dodgy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:green\">>>>RUN</span>\n",
    "\n",
    "Try again for a different set of data, called \"chisq_data2.txt\". Again, compute and print the Chi-squared coefficient and probability.\n",
    "\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = np.loadtxt('chisq_data2.txt', delimiter=',')    # read in file\n",
    "plt.xlim(-1,1)\n",
    "plt.xlabel(r'$\\cos (x)$')\n",
    "plt.plot(x,pred_y,linewidth=1,drawstyle='steps')\n",
    "plt.scatter(x,data2,c='k')\n",
    "plot_error(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2,p = chisquare(data2,pred_y)\n",
    "print(\"Chi-squared:\",chi2)\n",
    "print(\"Probability: {:.2%}\".format(p)) #quite consistent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:green\">>>>RUN</span>\n",
    "\n",
    "Try again for a different set of data, called \"chisq_data3.txt\". Again, compute and print the Chi-squared coefficient and probability.\n",
    "\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = np.loadtxt('chisq_data3.txt', delimiter=',')    # read in file\n",
    "plt.xlim(-1,1)\n",
    "plt.xlabel(r'$\\cos (x)$')\n",
    "plt.plot(x,pred_y,linewidth=1,drawstyle='steps')\n",
    "plt.scatter(x,data3,c='k')\n",
    "plot_error(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2,p = chisquare(data3,pred_y)\n",
    "print(\"Chi-squared:\",chi2)\n",
    "print(\"Probability: {:.2%}\".format(p)) #too consistent..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:purple\">>>>QUESTION</span>\n",
    "\n",
    "\n",
    "[WHAT WERE THEY TRYING TO LEARN?]\n",
    "\n",
    "<br>\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
