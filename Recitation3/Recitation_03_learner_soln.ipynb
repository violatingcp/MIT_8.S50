{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "<hr style=\"height: 1px;\">\n",
    "<i>This code was authored by the 8.S50x Course Team, Copyright 2021 MIT All Rights Reserved.</i>\n",
    "<hr style=\"height: 1px;\">\n",
    "<br>\n",
    "\n",
    "# RECITATION 3: Fitting, `LMFIT` Software, and Likelihoods\n",
    "\n",
    "<br>\n",
    "<!--end-block--> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## 3.0 Overview of Learning Objectives\n",
    "\n",
    "In this recitation we will explore the following objectives:\n",
    "\n",
    "- The principles of Frequentist fitting\n",
    "- The software package `LMFIT`\n",
    "- Likelihoods\n",
    "- An `LMFIT` example more related to your project\n",
    "\n",
    "The section on likelihoods also contains some practice with plotting distributions.\n",
    "\n",
    "<br>\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## 3.1 Principles of Frequentist Fitting\n",
    "\n",
    "Suppose you have some data $D$ you've collected, and a model for the data. Your model is really a function $f$ which maps from some model parameters $\\theta$ to the data $D$. The purpose of the fit is to determine the true value of $\\theta$ from the data.\n",
    "\n",
    "In this language, $D$ and $\\theta$ can be vectors or scalars depending on how many data points or parameters there are. To make things more complicated, the data points $D$ usually have an uncertainty on them called a <b>systematic uncertainty</b>. You can represent this by thinking of the data as a set of random variables.\n",
    "\n",
    "(<i>Technical note:</i> In practice, thinking of the data as random variables means that the parameter $\\theta$ is also a random variable. It therefore doesn't make sense to ask for the true <i>value</i> of $\\theta$; instead we should ask for the true <i>distribution</i>. However, most people gloss over this fact by representing the \"value\" of $\\theta$ as its distribution's mean.)\n",
    "\n",
    "\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "\n",
    "As you can imagine, the problem of finding the true $\\theta$ is hard. That's why there are multiple fitting methods. The most popular are **Bayesian** and **frequentist** fitting, and we study frequentist here.\n",
    "\n",
    "To fit a model function $f$ to data $D$ using the frequentist method, perform the following two steps.\n",
    "\n",
    "1. Use your knowledge of the model to derive a <b>likelihood</b> function for your data. Likelihood is defined as the probability of the data $D$ occuring given some guess at parameters $\\theta$. In the language of probability,\n",
    "$$\\mathcal{L}(\\theta) = P(D | \\theta)$$\n",
    "\n",
    "2. Find the value $\\theta_0$ which maximizes the likelihood. Take this value as your fit result.\n",
    "\n",
    "That's it! In practice, you might also be interested in getting uncertainties on your fit result, so we'll discuss that in the next section.\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:purple\">>>>QUESTION</span>\n",
    "\n",
    "\n",
    "The frequentist method is so simple! Why do other methods exist? I.e., what are potential problems with the logic behind the method?\n",
    "\n",
    "Hint: Why should the $\\theta$ that maximizes likelihood be the true $\\theta$? Should the goal be to maximize $P(D|\\theta)$ or $P(\\theta|D)$?\n",
    "\n",
    "<br>\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## 3.3 Using `LMFIT` to fit to data\n",
    "\n",
    "To get some practice fitting, suppose you have some data coming from the function $y=2x$.\n",
    "\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:green\">>>>RUN</span>\n",
    "\n",
    "Let's generate some example data with made up systematic uncertainties. These uncertainties are modeled as the standard deviations of normal distributions. Therefore, we draw each data point $y_i$ from a normal distribution with standard deviation equal to the uncertainty of point $i$ and mean $2 x_i$.\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "xi = np.array([2,3,4,5,6,7])\n",
    "yerr = np.array([0.3, 0.4, 0.45, 0.35, 0.6, 0.5])\n",
    "yi = 2*xi+yerr*np.random.normal(xi.shape)\n",
    "-->\n",
    "\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "xi = np.array([2,3,4,5,6,7])\n",
    "yerr = np.array([0.3, 0.4, 0.45, 0.35, 0.6, 0.5])\n",
    "yi = 2*xi+yerr*np.random.normal(xi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:#BA2220\">>>>EXERCISE</span>\n",
    "\n",
    "Plot the data and its error bars.\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#your code here\n",
    "-->\n",
    "\n",
    "<!--\n",
    "#solution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.errorbar(xi, yi, yerr=yerr, linestyle='none')\n",
    "plt.scatter(xi, yi)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.errorbar(xi, yi, yerr=yerr, linestyle='none')\n",
    "plt.scatter(xi, yi)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:green\">>>>RUN</span>\n",
    "\n",
    "We'll make a model using `LMFIT` to represent the data. Models can either be selected from a [large list of functions](https://lmfit.github.io/lmfit-py/builtin_models.html) already set up by `LMFIT`, or you can make them yourself. Here we use a preset linear model.\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "from lmfit.models import LinearModel\n",
    "model = LinearModel()\n",
    "-->\n",
    "\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmfit.models import LinearModel\n",
    "model = LinearModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:green\">>>>RUN</span>\n",
    "\n",
    "We'll skip the first set of fitting, using `LMFIT`'s automatic likelihood function (a chi-squared likelihood: see tomorrow's recitation). Let's jump straight to likelihood maximization. This maximization uses the same minimization methods you learned in class! Provide the data to `LMFIT`'s fit function.\n",
    "\n",
    "<b>Important:</b> set the weights equal to one over the systematic uncertainty. Not the uncertainty itself, and not the variance.\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "result = model.fit(yi, x=xi, weights=1/yerr);\n",
    "\n",
    "print(result.fit_report())\n",
    "-->\n",
    "\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.fit(yi, x=xi, weights=1/yerr);\n",
    "\n",
    "print(result.fit_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "\n",
    "Look in the Variables section: we have a slope of about 2 and an intercept of about zero, consistent with the true model! Very helpfully, `LMFIT` also gives you uncertainties on the fit parameters. These result from propagating the uncertainties of the data set.\n",
    "\n",
    "We'll talk about the chi-square and reduced chi-square statistics later.\n",
    "\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:green\">>>>RUN</span>\n",
    "\n",
    "There's one more thing to do: it's always a good idea to verify that your model actually fits your data. So let's plot the data together with the model.\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "result.plot();\n",
    "-->\n",
    "\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "\n",
    "You can see that the model fits the data very well in the bottom plot, and the top plot demonstrates that deviations of the data from the model are random; they don't seem correlated with $x$ nor with each other.\n",
    "\n",
    "\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## 3.3 Likelihood\n",
    "\n",
    "To investigate the first step of the minimization process, let's look at a different example.\n",
    "\n",
    "Suppose you're an astrophysicist looking at a distant star. Photons hit your telescope at random, independent intervals, so the the number that you detect within your period of observation is Poisson distributed.\n",
    "\n",
    "Also, this star is really important, and $N\\gg 1$ telescopes are looking at it. Your data $D$ is therefore is $\\{n_1, n_2, \\dots, n_N\\}$ which is the counts observed by the telescopes during one day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:green\">>>>RUN</span>\n",
    "\n",
    "Let's generate some Poisson-distributed sample data for each telescope. We'll assume a parameter of $\\lambda=5$ counts per day, with $N=100$ telescopes.\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "import numpy as np\n",
    "\n",
    "LAMBDA = 5\n",
    "N = 100\n",
    "\n",
    "counts = np.random.poisson(LAMBDA, N);\n",
    "-->\n",
    "\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "LAMBDA = 5\n",
    "N = 100\n",
    "\n",
    "counts = np.random.poisson(LAMBDA, N);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:green\">>>>RUN</span>\n",
    "\n",
    "Since each telescope's detection $n_i$ is independent, the probability of detecting data set $D$ given some estimate of $\\lambda$, which is the parameter, is simply the product of the probability for each telescope to detect $n_i$. This probability is Poisson distributed.\n",
    "\n",
    "We would like to use `LMFIT`'s minimize function, which does not actually request the likelihood as an input. Instead, it asks for the logarithm of the likelihood of each data point, and assumes that each data point is independent. Internally, it adds all the likelihoods from all data points to get the log-likelihood of the data.\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "\n",
    "def log_likelihood(l, data):\n",
    "    return np.log(poisson.pmf(data, l))\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "\n",
    "def log_likelihood(l, data):\n",
    "    return np.log(poisson.pmf(data, l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:green\">>>>RUN</span>\n",
    "\n",
    "Use `LMFIT`'s minimize function to maximize the likelihood (i.e. minimize the negative likelihood) and recover our $\\lambda=5$ value.\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "from lmfit import minimize, fit_report\n",
    "\n",
    "def negative_log_likelihood(l, data):\n",
    "    return -log_likelihood(l, data)\n",
    "\n",
    "params = Parameters()\n",
    "params.add('l', min=0, value=1)\n",
    "\n",
    "result = minimize(negative_log_likelihood, params, args=(counts,))\n",
    "print(fit_report(result))\n",
    "-->\n",
    "\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmfit import Parameters, minimize, fit_report\n",
    "\n",
    "def negative_log_likelihood(l, data):\n",
    "    return -log_likelihood(l, data)\n",
    "\n",
    "params = Parameters()\n",
    "params.add('l', min=0, value=1)\n",
    "\n",
    "result = minimize(negative_log_likelihood, params, args=(counts,))\n",
    "print(fit_report(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:purple\">>>>QUESTION</span>\n",
    "\n",
    "\n",
    "Why is it okay to use the log likelihood instead of the likelihood for our likelihood function?\n",
    "\n",
    "Hint: what is the one and only purpose of the likelihood function we have mentioned so far? How is it affected by taking the log?\n",
    "\n",
    "<br>\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:#BA2220\">>>>EXERCISE</span>\n",
    "\n",
    "Show the following in the same plot\n",
    "* A histogram of the $n_i$ values observed by all the telescopes (choose your bins wisely)\n",
    "* Superimposed, the best fit Poisson distribution with the $\\lambda$ you got above (access the $\\lambda$ with `result.params['l'].value` instead of copying it over)\n",
    "* The true distribution ($\\lambda$=5)\n",
    "* Axis labels\n",
    "* A legend\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "\n",
    "#your code here\n",
    "-->\n",
    "\n",
    "<!--\n",
    "#solution\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "\n",
    "bins = np.arange(np.max(counts)+1)-0.5\n",
    "xs = bins+0.5\n",
    "plt.hist(counts, bins=bins, label=\"Photon data\", fill=False, histtype=\"step\", color='k', linewidth=2)\n",
    "plt.plot(xs, N * poisson.pmf(xs, result.params['l'].value), label=\"Best fit distro\", color=\"C0\", linewidth=2)\n",
    "plt.plot(xs, N * poisson.pmf(xs,LAMBDA), label=\"True distro\", color=\"C1\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Number of photons observed\")\n",
    "plt.ylabel(\"Number of telescopes\")\n",
    "plt.legend()\n",
    "-->\n",
    "\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "\n",
    "bins = np.arange(np.max(counts)+1)-0.5\n",
    "xs = bins+0.5\n",
    "plt.hist(counts, bins=bins, label=\"Photon data\", fill=False, histtype=\"step\", color='k', linewidth=2)\n",
    "plt.plot(xs, N * poisson.pmf(xs, result.params['l'].value), label=\"Best fit distro\", color=\"C0\", linewidth=2)\n",
    "plt.plot(xs, N * poisson.pmf(xs,LAMBDA), label=\"True distro\", color=\"C1\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Number of photons observed\")\n",
    "plt.ylabel(\"Number of telescopes\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:#BA2220\">>>>EXERCISE</span>\n",
    "\n",
    "To take into effect the uncertainty on $\\lambda$, compute the Poisson distribution not just for the best fit value of $\\lambda$, but also for $\\lambda - 2\\sigma_\\lambda$, $\\lambda+2\\sigma_\\lambda$, and ten $\\lambda$s in between. Store those results in a list. Here, $\\sigma_\\lambda$ is the uncertainty on $\\lambda$ generated by the fit and you can get it with `result.params['l'].stderr`.\n",
    "\n",
    "Copy and paste your code from above and add an error band (`plt.fill_between`) where the lower edge of the error band in each bin represents the lowest number of telescopes predicted among all the distributions you computed above, and the higher edge represents the highest number of telescopes.\n",
    "\n",
    "Please ask for help if you're confused about how to do this or about what you're being asked to do.\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "\n",
    "#your code here\n",
    "-->\n",
    "\n",
    "<!--\n",
    "#solution\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "\n",
    "bins = np.arange(np.max(counts)+1)-0.5\n",
    "xs = bins+0.5\n",
    "plt.hist(counts, bins=bins, label=\"Photon data\", fill=False, histtype=\"step\", color='k', linewidth=2)\n",
    "plt.plot(xs, N * poisson.pmf(xs, result.params['l'].value), label=\"Best fit distro\", color=\"C0\", linewidth=2)\n",
    "plt.plot(xs, N * poisson.pmf(xs,LAMBDA), label=\"True distro\", color=\"C1\", linewidth=2)\n",
    "\n",
    "factor = np.linspace(-2, 2, 10)\n",
    "distros = np.array([N * poisson.pmf(xs, result.params['l'].value + f * result.params['l'].stderr) for f in factor])\n",
    "minimum = [np.min(distros[:,i]) for i in range(distros.shape[1])]\n",
    "maximum = [np.max(distros[:,i]) for i in range(distros.shape[1])]\n",
    "plt.fill_between(xs, minimum, maximum, alpha=0.5, color=\"C0\")\n",
    "plt.xlabel(\"Number of photons observed\")\n",
    "plt.ylabel(\"Number of telescopes\")\n",
    "plt.legend()\n",
    "-->\n",
    "\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "\n",
    "bins = np.arange(np.max(counts)+1)-0.5\n",
    "xs = bins+0.5\n",
    "plt.hist(counts, bins=bins, label=\"Photon data\", fill=False, histtype=\"step\", color='k', linewidth=2)\n",
    "plt.plot(xs, N * poisson.pmf(xs, result.params['l'].value), label=\"Best fit distro\", color=\"C0\", linewidth=2)\n",
    "plt.plot(xs, N * poisson.pmf(xs,LAMBDA), label=\"True distro\", color=\"C1\", linewidth=2)\n",
    "\n",
    "factor = np.linspace(-2, 2, 10)\n",
    "distros = np.array([N * poisson.pmf(xs, result.params['l'].value + f * result.params['l'].stderr) for f in factor])\n",
    "minimum = [np.min(distros[:,i]) for i in range(distros.shape[1])]\n",
    "maximum = [np.max(distros[:,i]) for i in range(distros.shape[1])]\n",
    "plt.fill_between(xs, minimum, maximum, alpha=0.5, color=\"C0\")\n",
    "plt.xlabel(\"Number of photons observed\")\n",
    "plt.ylabel(\"Number of telescopes\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## 3.4 A more relevant LMFIT example\n",
    "\n",
    "Let's apply `LMFIT` in a setting more related to your project: we'll investigate a famous LIGO signal.\n",
    "\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:green\">>>>RUN</span>\n",
    "\n",
    "Download the signature of the first black hole detected (GW150914) from [the LIGO data archive]() and save it to the directory of this notebook. Then run this code.\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "-->\n",
    "\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import gwfreq, osc_scale, osc, lower, higher\n",
    "from gwpy.timeseries import TimeSeries\n",
    "import h5py\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import os\n",
    "\n",
    "fn = 'H-H1_GWOSC_16KHZ_R1-1126257415-4096.hdf5' # data file\n",
    "tevent = 1126259462.422 # Mon Sep 14 09:50:45 GMT 2015\n",
    "evtname = 'GW150914' # event name\n",
    "\n",
    "detector = 'H1' # detecotr: L1 or H1\n",
    "\n",
    "strain = TimeSeries.read(fn, format='hdf5.losc')\n",
    "center = int(tevent)\n",
    "strain = strain.crop(center-16, center+16)\n",
    "asd = strain.asd(fftlength = 1)\n",
    "\n",
    "NRtime, NR_H1 = np.genfromtxt('GW150914_4_NR_waveform.txt').transpose()\n",
    "nrdata= TimeSeries(NR_H1, times = NRtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that whitens and filters the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whiten_and_bandpass(strain, asd, bp_low, bp_high):\n",
    "    fft = strain.fft()\n",
    "    asdinterp = interp1d(asd.frequencies, asd)\n",
    "    asddiv =asdinterp(fft.frequencies)\n",
    "    white_freq = fft/asddiv\n",
    "    white = white_freq.ifft()\n",
    "    whitebp = white.bandpass(bp_low,bp_high)\n",
    "    return TimeSeries(whitebp, t0=strain.t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_pass = lower()\n",
    "high_pass = higher()\n",
    "\n",
    "whitened_data = whiten_and_bandpass(strain, asd, low_pass, high_pass)\n",
    "wnr = whiten_and_bandpass(nrdata, asd, low_pass, high_pass).crop(-0.1, 0.05)\n",
    "zoom = whitened_data.crop(tevent-0.09, tevent+0.05)\n",
    "zoom.t0 = -0.09\n",
    "x = np.array(zoom.times)\n",
    "white_data_bp_zoom = zoom.value\n",
    "plt.plot(x, white_data_bp_zoom)\n",
    "plt.plot(wnr.times, wnr.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a funciton that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define osc_dif for lmfit::minimize()\n",
    "def osc_dif(params, x, data, data_asd, low_pass, high_pass):\n",
    "    iM=params[\"Mc\"]\n",
    "    iT0=params[\"t0\"]\n",
    "    norm=params[\"C\"]\n",
    "    phi=params[\"phi\"]\n",
    "    model_strain = TimeSeries(osc(x, iM, iT0, phi, norm), times = x)\n",
    "    val = whiten_and_bandpass(model_strain, data_asd, low_pass, high_pass)\n",
    "    residuals = val-data\n",
    "    return residuals\n",
    "\n",
    "def osc_white(t, Mc, t0, phi, C, data_asd=None, low_pass = None, high_pass = None):\n",
    "    model_strain = TimeSeries(osc(t, Mc, t0, phi, C), times = t)\n",
    "    val = whiten_and_bandpass(model_strain, data_asd, low_pass, high_pass) \n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owmodel = lmfit.Model(osc_white, data_asd = asd, low_pass = low_pass, high_pass = high_pass)\n",
    "owmodel.set_param_hint(name = 'Mc', value = 25, min = 10, max = 30)\n",
    "owmodel.set_param_hint(name = 't0', value = 0.004, min = -0.015, max = 0.015)\n",
    "owmodel.set_param_hint(name = 'C', value = 0.18853556, min = 0.01, max = 10)\n",
    "owmodel.set_param_hint(name = 'phi', value = 1.24685424, min = 0, max = 2*np.pi)\n",
    "\n",
    "omodel = lmfit.Model(osc_scale)\n",
    "omodel.set_param_hint(name = 'Mc', value = 25)\n",
    "omodel.set_param_hint(name = 't0', value = 0.01)\n",
    "omodel.set_param_hint(name = 'C', value = 0.18853556)\n",
    "omodel.set_param_hint(name = 'phi', value = 1.24685424)\n",
    "\n",
    "result_white = owmodel.fit(white_data_bp_zoom, t = x)\n",
    "print(result_white.fit_report())\n",
    "result_white.plot(datafmt='-')\n",
    "\n",
    "result = omodel.fit(white_data_bp_zoom, t = x)\n",
    "print(result.fit_report())\n",
    "result.plot(datafmt='-')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmfit\n",
    "from lmfit import Model, minimize, fit_report, Parameters\n",
    "\n",
    "model = lmfit.Model(osc_white)\n",
    "p = model.make_params()\n",
    "p['Mc'].set(25)     # Mass guess\n",
    "p['t0'].set(-0.005)  # By construction we put the merger in the center\n",
    "p['C'].set(0.18853556)      # normalization guess\n",
    "p['phi'].set(1.24685424)    # Phase guess\n",
    "out = minimize(osc_dif, params=p, args=(x, white_data_bp_zoom, asd, low_pass, high_pass))\n",
    "print(fit_report(out))\n",
    "best_fit = model.eval(params=out.params,t=x, data_asd = asd, low_pass = low_pass, high_pass = high_pass)\n",
    "init_fit = model.eval(params=p,t=x, data_asd = asd, low_pass = low_pass, high_pass = high_pass)\n",
    "plt.plot(x, best_fit,'r',label='best fit')\n",
    "#plt.plot(x, init_fit,'b',label='init')\n",
    "plt.plot(x, white_data_bp_zoom)\n",
    "plt.plot(wnr.times, wnr.value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:purple\">>>>QUESTION</span>\n",
    "\n",
    "\n",
    "Question text needed...\n",
    "\n",
    "\n",
    "<br>\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
