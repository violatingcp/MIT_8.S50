{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecb57d5",
   "metadata": {},
   "source": [
    "# VQQ Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953577f6",
   "metadata": {},
   "source": [
    "\n",
    "**Author: Duc Hoang,  Philip Harris (MIT)**\n",
    "\n",
    "**Date: 9/1/2022**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b29390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's import what we need for our lab first\n",
    "# uproot High energy physics python file format => https://masonproffitt.github.io/uproot-tutorial/aio.html\n",
    "import uproot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.stats as stats\n",
    "\n",
    "#plotting style for High Energy physics \n",
    "import lmfit as lm\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.style.CMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0509af",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f19df7",
   "metadata": {},
   "source": [
    "Here is some codes to load the correct data sets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60259932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets look at the data. Our data sample is the JetHT dataset. \n",
    "# What that means is the data passed triggers that have a jet in one of the triggers. (discuss below)\n",
    "data   = uproot.open(\"data/JetHT_s.root\")[\"Tree\"]\n",
    "\n",
    "# In addition to above we have Monte Carlo Simulation of many processes\n",
    "# Some of these process are well modelled in simulation and some of them are not\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Now we have our actual process qq=>W=>qq at 8TeV collision energy\n",
    "wqq    = uproot.open(\"data/WQQ_s.root\")[\"Tree\"]\n",
    "\n",
    "# Now we have our actual process qq=>Z=>qq at 8TeV collision energy\n",
    "zqq    = uproot.open(\"data/ZQQ_s.root\")[\"Tree\"] \n",
    "\n",
    "#Hint: You could check for files in the data directory by doing \"!ls data/\" in a cell.\n",
    "#The ZQQ file name is similar to WQQ\n",
    "\n",
    "# Unfortunately the samples I made above a long time ago are very small. \n",
    "# To train NNs and make nice plots we will use larger samples produced at a different collision energy\n",
    "# qq=>W=>qq at 13TeV collision energy\n",
    "wqq13  = uproot.open(\"data/skimh/WQQ_sh.root\")[\"Tree\"]\n",
    "\n",
    "# qq=>Z=>qq at 13TeV collision energy\n",
    "zqq13  = uproot.open(\"data/skimh/ZQQ_sh.root\")[\"Tree\"]\n",
    "\n",
    "# Now we have our worst modeled background this is also our main background. \n",
    "# This is is our di-jet quark and gluon background. \n",
    "# We just call these backgrounds QCD because they are produced with Quantum Chromo Dynamics. \n",
    "qcd    = uproot.open(\"data/QCD_s.root\")[\"Tree\"]\n",
    "\n",
    "# Now we have the Higgs boson sample (we might need this in the future)\n",
    "ggh    = uproot.open(\"data/ggH.root\")[\"Tree\"]\n",
    "\n",
    "# And top-quark pair production background. \n",
    "tt     = uproot.open(\"data/TT.root\")[\"Tree\"]\n",
    "\n",
    "# Finally we have the rarer double W, W+Z and Z+Z diboson samples where we have two bosons instead of one\n",
    "ww     = uproot.open(\"data/WW.root\")[\"Tree\"]\n",
    "wz     = uproot.open(\"data/WZ.root\")[\"Tree\"]\n",
    "zz     = uproot.open(\"data/ZZ.root\")[\"Tree\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd0640",
   "metadata": {},
   "source": [
    "## Simple helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f273e",
   "metadata": {},
   "source": [
    "After loading the data, you are provided with some simple helper functions that have already been used in our introduction notebook. These are used for pre-selection (standard cuts that physicsists usually apply before making measurements) and computing the scaling factor of datasets. You can review the previous notebook for further explanations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6d9f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection(iData):\n",
    "    '''\n",
    "    Standard pre-selection\n",
    "    '''\n",
    "    #lets apply a trigger selection\n",
    "    trigger = (iData.arrays('trigger', library=\"np\")[\"trigger\"].flatten() > 0)\n",
    "\n",
    "    #Now lets require the jet pt to be above a threshold (400 TODO: ASK about units)\n",
    "    jetpt   = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten() > 400)\n",
    "\n",
    "    #Lets apply both jetpt and trigger at the same time\n",
    "    #standard_trig = (iData.arrays('trigger', library=\"np\")[\"trigger\"].flatten() % 4 > 1) #lets require one of our standard triggers (jet pT > 370 )\n",
    "    allcuts = np.logical_and.reduce([trigger,jetpt])\n",
    "\n",
    "    return allcuts\n",
    "    \n",
    "def get_weights(iData,weights,sel):\n",
    "    \n",
    "    weight = weights[0]\n",
    "    \n",
    "    for i in range(1,len(weights)):\n",
    "        weight *= iData.arrays(weights[i],library=\"np\")[weights[i]][sel]\n",
    "        \n",
    "    return weight\n",
    "\n",
    "def integral(iData,iWeights):\n",
    "    '''\n",
    "    This computs the integral of weighted events \n",
    "    assuming a selection given by the function selection (see below)\n",
    "    '''\n",
    "    \n",
    "    #perform a selection on the data (\n",
    "    mask_sel=selection(iData)\n",
    "    \n",
    "    #now iterate over the weights not the weights are in the format of [number,variable name 1, variable name 2,...]\n",
    "    weight  =iWeights[0]\n",
    "    \n",
    "    for i0 in range(1,len(iWeights)):\n",
    "        weightarr = iData.arrays(iWeights[i0], library=\"np\")[iWeights[i0]][mask_sel].flatten()\n",
    "        \n",
    "        #multiply the weights\n",
    "        weight    = weight*weightarr\n",
    "    \n",
    "    #now take the integral and return it\n",
    "    return np.sum(weight)\n",
    "\n",
    "\n",
    "def scale(iData8TeV,iData13TeV,iWeights):\n",
    "    '''\n",
    "    This computes the integral of two selections for two datasets labelled 8TeV and 13TeV,\n",
    "    but really can be 1 and 2. Then it returns the ratio of the integrals\n",
    "    '''\n",
    "    \n",
    "    int_8TeV  = integral(iData8TeV,iWeights)\n",
    "    int_13TeV = integral(iData13TeV,iWeights)\n",
    "    \n",
    "    return int_8TeV/int_13TeV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c748f3",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d0e007",
   "metadata": {},
   "source": [
    "# 0. Find the W Peak\n",
    "\n",
    "In this section, we provide you with some example codes to investigate the W peak. In the next section, we will ask you to fit it. First, let's make a plot of the data without any cuts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aea605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDataSim(iVar, iSelection, iVarName, iRange):\n",
    "    \n",
    "    #Lets Look at the mass\n",
    "    weights = [1000*18300, \"puweight\", \"scale1fb\"]\n",
    "    mrange = iRange #range for mass histogram [GeV]\n",
    "    bins=40            #bins for mass histogram\n",
    "    density = False     #to plot the histograms as a density (integral=1)\n",
    "\n",
    "    qcdsel      = iSelection(qcd)\n",
    "    wsel        = iSelection(wqq13)\n",
    "    zsel        = iSelection(zqq13)\n",
    "    datasel     = iSelection(data)\n",
    "    ttsel       = iSelection(tt)\n",
    "    wwsel       = iSelection(ww)\n",
    "    wzsel       = iSelection(wz)\n",
    "    zzsel       = iSelection(zz)\n",
    "    gghsel      = iSelection(ggh)\n",
    "\n",
    "    wscale=scale(wqq,wqq13,weights)\n",
    "    zscale=scale(zqq,zqq13,weights)\n",
    "\n",
    "    # Getting the masses of selected events\n",
    "    dataW = data.arrays(iVar, library=\"np\") [iVar][datasel]\n",
    "    qcdW  = qcd.arrays(iVar, library=\"np\")  [iVar][qcdsel]\n",
    "    wW    = wqq13.arrays(iVar, library=\"np\")[iVar][wsel]\n",
    "    zW    = zqq13.arrays(iVar, library=\"np\")[iVar][zsel]\n",
    "    zzW   = zz   .arrays(iVar, library=\"np\")[iVar][zzsel]\n",
    "    wzW   = wz   .arrays(iVar, library=\"np\")[iVar][wzsel]\n",
    "    wwW   = ww   .arrays(iVar, library=\"np\")[iVar][wwsel]\n",
    "    ttW   = tt   .arrays(iVar, library=\"np\")[iVar][ttsel]\n",
    "    gghW  = ggh  .arrays(iVar, library=\"np\")[iVar][gghsel]\n",
    "\n",
    "    #Define the weights for the histograms\n",
    "    hist_weights = [get_weights(qcd,weights,qcdsel),\n",
    "                    get_weights(wqq13,weights,wsel)*wscale,\n",
    "                    get_weights(zqq13,weights,zsel)*zscale,\n",
    "                    get_weights(zz,weights,zzsel),\n",
    "                    get_weights(wz,weights,wzsel),\n",
    "                    get_weights(ww,weights,wwsel),\n",
    "                    get_weights(tt,weights,ttsel),\n",
    "                   ]\n",
    "\n",
    "    #Hint: Provide a list of selected data\n",
    "    plt.hist([qcdW,wW, zW, zzW, wzW, wwW, ttW],\n",
    "             color=[\"royalblue\",\"r\", \"orange\",\"g\", \"b\", \"purple\", \"cyan\",], \n",
    "             label=[\"QCD\", \"W\", \"Z\", \"ZZ\", \"WZ\", \"WW\", \"tt\",], \n",
    "             weights=hist_weights,\n",
    "             range=mrange, bins=50, alpha=.6, density=density,stacked=True)\n",
    "\n",
    "    #Other configurations for the histogram\n",
    "    counts, bins = np.histogram(dataW, bins=bins, range=mrange, density=density)\n",
    "    yerr = np.sqrt(counts) / np.sqrt(len(dataW)*np.diff(bins))\n",
    "    binCenters = (bins[1:]+bins[:-1])*.5\n",
    "    plt.errorbar(binCenters, counts, yerr=yerr,fmt=\"o\",c=\"k\",label=\"data\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(iVarName)\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.show()\n",
    "\n",
    "plotDataSim(\"vjet0_msd0\", selection, \"Jet Mass\", [40,200])\n",
    "plotDataSim(\"vjet0_t2\", selection, r\"$\\tau_2$\", [0,0.5]) \n",
    "#Add some code here to compare variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299d04bc",
   "metadata": {},
   "source": [
    "When we want to compare variables, we plot the variable, the optimal cut is often at the point where the signal and background cross. As a example look at the plot below, we see that the $\\tau_2/\\tau_1$ point cuts at around 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856cd0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tau21():\n",
    "    \n",
    "    #Lets Look at the mass\n",
    "    weights = [1000*18300, \"puweight\", \"scale1fb\"]\n",
    "\n",
    "    qcdsel      = selection(qcd)\n",
    "    wsel        = selection(wqq13)\n",
    "\n",
    "    wscale=scale(wqq,wqq13,weights)\n",
    "\n",
    "    # Getting the masses of selected events\n",
    "    qcd_t21  = (qcd.arrays('vjet0_t2', library=\"np\")['vjet0_t2'][qcdsel]/\n",
    "               qcd.arrays('vjet0_t1', library=\"np\")['vjet0_t1'][qcdsel])\n",
    "    \n",
    "    w_t21    = (wqq13.arrays('vjet0_t2', library=\"np\")['vjet0_t2'][wsel]/\n",
    "               wqq13.arrays('vjet0_t1', library=\"np\")['vjet0_t1'][wsel])\n",
    "\n",
    "    plt.hist(qcd_t21,\n",
    "             weights=get_weights(qcd,weights,qcdsel),\n",
    "             bins=50,\n",
    "             color='red',\n",
    "             label=\"QCD\", alpha=.6, density = True)\n",
    "    \n",
    "    plt.hist(w_t21,\n",
    "             weights=get_weights(wqq13,weights,wsel)*wscale,\n",
    "             bins=50,\n",
    "             color='black',\n",
    "             label=\"W\", alpha=.6, density = True)\n",
    "\n",
    "    #Other configurations for the histogram\n",
    "    plt.legend()\n",
    "    plt.xlabel(r\"$\\tau_{2}/\\tau_{1}$\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.show()\n",
    "\n",
    "plot_tau21()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectionW(iData):\n",
    "    '''\n",
    "    This is the specific selection for selecting out events with W signal for our analysis\n",
    "    '''\n",
    "    \n",
    "    #Pre-selection citeria\n",
    "    trigger = (iData.arrays('trigger', library=\"np\")[\"trigger\"].flatten() >= 0)\n",
    "    jetpt   = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten() >= 400)\n",
    "    \n",
    "    #Select the jets to compute tau2/tau1\n",
    "    jett2   = (iData.arrays('vjet0_t2', library=\"np\")[\"vjet0_t2\"].flatten())\n",
    "    jett1   = (iData.arrays('vjet0_t1', library=\"np\")[\"vjet0_t1\"].flatten())\n",
    "    \n",
    "    t21 = jett2/jett1\n",
    "                                \n",
    "    #And then perform the cut\n",
    "    #Hint: You could determine the threshold of the cut by plotting the distribution of \n",
    "    #t21ddt scores for W and background and then determine a ball park threshold\n",
    "    #where you think the W signal would be best selected\n",
    "    #Or more simply you could look at the given plot and determine the appropriate threshold.\n",
    "    \n",
    "    t21cut   = t21 < 0.5\n",
    "    \n",
    "    allcuts = np.logical_and.reduce([trigger, jetpt, t21cut])\n",
    "    \n",
    "    return allcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc27580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDataSim(\"vjet0_msd0\", selectionW, \"Jet Mass\",[40,200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f837c3",
   "metadata": {},
   "source": [
    "Looking at this plot, we can see that the W peak is not at all obvious to find! This is why we need to employ additional techniques in order to clearly identify the W peak, which is what you'll have the chance to do in the next section!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6e986",
   "metadata": {},
   "source": [
    "# 1. Fit for W Peak\n",
    "\n",
    "Now, lets do the lab. <font color=\"blue\"> Your first challenge is to make a mass plot and perform fitting for W signal. </font> A hint is that the plots should come out similarly to this (it doesn't have to be exactly the same), where on the left we show the soft-drop mass/groomed mass ($m_{SD}$) distribution for different processes in the Monte Carlo simulation along with the real data. The plot on the right shows the fit:\n",
    "\n",
    "<img src=\"images/S50_WFit.png\" width='900'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4396cf64",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2586d7",
   "metadata": {},
   "source": [
    "## 1.1 W Mass Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e616cd",
   "metadata": {},
   "source": [
    "Since finding W peak is hard, we need to use another parameter, $\\rho$, which is a scaling variable for QCD jets. This parameter adds another channels of mass and $p_T$ to our selection, helping us to refine our W peak.\n",
    "\n",
    "$\\rho$ is defined in this paper:\n",
    "\n",
    "https://arxiv.org/pdf/1603.00027.pdf\n",
    "\n",
    "Your first goal is to figure out how $\\rho$ is defined by quoting the paper, and then figure out the best selections based on a combination of $\\rho$ and $\\tau_2/\\tau_1$. The final cut is based on a parameter defined as *DT* (Deccorelated Taggers) score:\n",
    "\n",
    "$$(\\tau_2/\\tau_1)_{dt} = \\tau_2/\\tau_1 - (\\text{your correlation})*\\rho$$\n",
    "\n",
    "Where the correlation **<em> your correlation </em>** is the correlation coefficient between $\\tau_2/\\tau_1$ and $\\rho$. \n",
    "\n",
    "To figure out the correlation, let's plot $\\tau_2/\\tau_1$ and $\\rho$ in the data first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2880b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_taus_and_rho(iData):\n",
    "    \n",
    "    jetptnocut = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten())\n",
    "    jetmass = (iData.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"].flatten())\n",
    "    jett2   = (iData.arrays('vjet0_t2', library=\"np\")[\"vjet0_t2\"].flatten())\n",
    "    jett1   = (iData.arrays('vjet0_t1', library=\"np\")[\"vjet0_t1\"].flatten())\n",
    "    \n",
    "    #Define rho according to the paper\n",
    "    rho = # Definition from the paper\n",
    "    \n",
    "    #Define tau2/tau1\n",
    "    t21 = # The ratio\n",
    "    \n",
    "    plt.hist2d(rho, t21, bins = 40)\n",
    "    \n",
    "    plt.xlabel(r\"$\\rho$\")\n",
    "    plt.ylabel(r\"$\\tau_2/\\tau_1$\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_taus_and_rho(qcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac625cf",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffde071",
   "metadata": {},
   "source": [
    "Great! Now we can fit a line on the 2D histogram to determine the correlation! Here we give you the fitting code. The codes fit by putting a threshold on the 2D histogram to selectively fit on the most relevant data points, your task for this is to play around with the threshold to determine the best fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_correlation(iData):\n",
    "    \n",
    "    jetptnocut = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten())\n",
    "    jetmass = (iData.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"].flatten())\n",
    "    jett2   = (iData.arrays('vjet0_t2', library=\"np\")[\"vjet0_t2\"].flatten())\n",
    "    jett1   = (iData.arrays('vjet0_t1', library=\"np\")[\"vjet0_t1\"].flatten())\n",
    "    \n",
    "    #Define rho according to the paper\n",
    "    rho = np.log(jetmass**2/jetptnocut)\n",
    "    \n",
    "    #Define tau2/tau1\n",
    "    t21 = jett2/jett1\n",
    "    \n",
    "    plt.hist2d(rho, t21, bins = 40)\n",
    "    plt.xlabel(r\"$\\rho$\")\n",
    "    plt.ylabel(r\"$\\tau_2/\\tau_1$\")\n",
    "    \n",
    "    #Fit the line\n",
    "    #Produce 2D histogram\n",
    "    H,xedges,yedges = np.histogram2d(rho,t21, bins=40,density = True)\n",
    "    \n",
    "    bin_centers_x = (xedges[:-1]+xedges[1:])/2.0\n",
    "    bin_centers_y = (yedges[:-1]+yedges[1:])/2.0\n",
    "    \n",
    "    #Find the non-zero indicies\n",
    "    non_zero_idx = np.argwhere(H > 0.6) #You can play around with this!\n",
    "    x_idx = non_zero_idx[:,0]\n",
    "    y_idx = non_zero_idx[:,1]\n",
    "    \n",
    "    x_coord = [bin_centers_x[x_idx[i]] for i in range(0,len(x_idx))]\n",
    "    y_coord = [bin_centers_y[y_idx[i]] for i in range(0,len(y_idx))]\n",
    "    \n",
    "    #Fit a linear model on the points plotted\n",
    "    def func(x, a, b):\n",
    "        return a * x + b\n",
    "    plt.scatter(x_coord, y_coord)\n",
    "    \n",
    "    popt, pcov = curve_fit(func, x_coord, y_coord)\n",
    "    plt.plot(bin_centers_x, func(bin_centers_x, *popt), 'r-',\n",
    "             label='fit: a=%5.3f, b=%5.3f' % tuple(popt))\n",
    "    \n",
    "    #Show the fit result\n",
    "    legend = plt.legend()\n",
    "    plt.setp(legend.get_texts(), color='w')\n",
    "    plt.show()\n",
    "\n",
    "fit_correlation(qcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2af4d7",
   "metadata": {},
   "source": [
    "Now use the correlation to define $(\\tau_2/\\tau_1)_{dt}$ and plot it with $\\rho$ to verify that we have successfully decorrelate the tagger! (you should see a straight-line distribution in the histogram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf25982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tausdt_and_rho(iData):\n",
    "    \n",
    "    jetptnocut = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten())\n",
    "    jetmass = (iData.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"].flatten())\n",
    "    jett2   = (iData.arrays('vjet0_t2', library=\"np\")[\"vjet0_t2\"].flatten())\n",
    "    jett1   = (iData.arrays('vjet0_t1', library=\"np\")[\"vjet0_t1\"].flatten())\n",
    "    \n",
    "    #Define rho according to the paper\n",
    "    rho = np.log(jetmass**2/jetptnocut)\n",
    "    \n",
    "    #Define tau2/tau1\n",
    "    t21 = jett2/jett1\n",
    "    \n",
    "    #decorrelated tagger score\n",
    "    t21dt = #Your decorrelated tagger score definition\n",
    "    \n",
    "    plt.hist2d(rho, t21dt, bins = 40)\n",
    "    \n",
    "    plt.xlabel(r\"$\\rho$\")\n",
    "    plt.ylabel(r\"$\\tau_2/\\tau_1$_dt\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_tausdt_and_rho(qcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1983c7",
   "metadata": {},
   "source": [
    "Great! if you do it correctly, you can see that the decorrelated scores are now independent of $\\rho$! Since you figured out your decorrelation, determine the best cut for the decorrelated taggers score (maybe by plotting the distribution between qcd background and the W signal like we did above) and use it in your selection function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectionW(iData):\n",
    "    '''\n",
    "    This is the specific selection for selecting out events with W signal for our analysis\n",
    "    '''\n",
    "    \n",
    "    #Pre-selection citeria\n",
    "    trigger = (iData.arrays('trigger', library=\"np\")[\"trigger\"].flatten() >= 0)\n",
    "    jetpt   = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten() >= 400)\n",
    "    jetptnocut = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten())\n",
    "    jetmass = (iData.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"].flatten())\n",
    "    jett2   = (iData.arrays('vjet0_t2', library=\"np\")[\"vjet0_t2\"].flatten())\n",
    "    jett1   = (iData.arrays('vjet0_t1', library=\"np\")[\"vjet0_t1\"].flatten())\n",
    "    \n",
    "    \n",
    "    #Define the two parameters rho and tau2/tau1 \n",
    "    rho = ### YOUR CODE HERE ###\n",
    "    t21 = ### YOUR CODE HERE ###\n",
    "    t21ddt = ### YOUR CODE HERE ###\n",
    "                            \n",
    "    #And then perform the cut\n",
    "    #Hint: You could determine the threshold of the cut by plotting the distribution of \n",
    "    #t21ddt scores for W and background and then determine a ball park threshold\n",
    "    #where you think the W signal would be best selected\n",
    "    t21cut   = ### YOUR CODE HERE ###\n",
    "    \n",
    "    allcuts = np.logical_and.reduce([trigger, jetpt, t21cut])\n",
    "    \n",
    "    return allcuts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa9b817",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542cd434",
   "metadata": {},
   "source": [
    "Now that we have our selection function, let's try to make the mass plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24447b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Look at the mass\n",
    "weights = [1000*18300, \"puweight\", \"scale1fb\"]\n",
    "mrange = (45,200)  #range for mass histogram [GeV]\n",
    "bins=40            #bins for mass histogram\n",
    "density = True     #to plot the histograms as a density (integral=1)\n",
    "\n",
    "qcdsel      = selectionW(qcd)\n",
    "wsel        = selectionW(wqq13)\n",
    "zsel        = selectionW(zqq13)\n",
    "datasel     = selectionW(data)\n",
    "ttsel       = selectionW(tt)\n",
    "wwsel       = selectionW(ww)\n",
    "wzsel       = selectionW(wz)\n",
    "zzsel       = selectionW(zz)\n",
    "gghsel      = selectionW(ggh)\n",
    "wscale=scale(wqq,wqq13,weights)\n",
    "zscale=scale(zqq,zqq13,weights)\n",
    "\n",
    "dataW = data.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][datasel]\n",
    "qcdW = qcd.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][qcdsel]\n",
    "wW = wqq13.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][wsel]\n",
    "zW = zqq13.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][zsel]\n",
    "zzW = zz.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][zzsel]\n",
    "wzW = wz.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][wzsel]\n",
    "wwW = ww.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][wwsel]\n",
    "ttW = tt.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][ttsel]\n",
    "gghW = ggh.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][gghsel]\n",
    "\n",
    "hist_weights = [get_weights(qcd,weights,qcdsel),\n",
    "                get_weights(wqq13,weights,wsel)*wscale,\n",
    "                get_weights(zqq13,weights,zsel)*zscale,\n",
    "                get_weights(zz,weights,zzsel),\n",
    "                get_weights(wz,weights,wzsel),\n",
    "                get_weights(ww,weights,wwsel),\n",
    "                get_weights(tt,weights,ttsel),\n",
    "               ]\n",
    "\n",
    "plt.hist([qcdW,wW, zW, zzW, wzW, wwW, ttW], \n",
    "         color=[\"royalblue\",\"r\", \"orange\",\"g\", \"b\", \"purple\", \"cyan\",], \n",
    "         label=[\"QCD\", \"W\", \"Z\", \"ZZ\", \"WZ\", \"WW\", \"tt\",], \n",
    "         weights=hist_weights,\n",
    "         range=mrange, bins=50, alpha=.6, density=density,stacked=True)\n",
    "\n",
    "counts, bins = np.histogram(dataW, bins=bins, range=mrange, density=density)\n",
    "yerr = np.sqrt(counts) / np.sqrt(len(dataW)*np.diff(bins))\n",
    "binCenters = (bins[1:]+bins[:-1])*.5\n",
    "plt.errorbar(binCenters, counts, yerr=yerr,fmt=\"o\",c=\"k\",label=\"data\")\n",
    "plt.legend()\n",
    "plt.xlabel(r\"Mass [GeV]\")\n",
    "plt.ylabel(\"Normalized Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13c522e",
   "metadata": {},
   "source": [
    "Remember to compare your mass plot with the one given above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2f29a",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca81e8",
   "metadata": {},
   "source": [
    "## 1.2 W Peak Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba0c25",
   "metadata": {},
   "source": [
    "In this section you will perform the fit on W signal. It involves a few steps:\n",
    "    \n",
    "1. First you need to define a fit model of your own. In this case we would use some functions (gaussian, exponential) in conjuntion with a 6th order polynomial. You could see more on how the order of the polynomials are determined here: https://en.wikipedia.org/wiki/Chow_test. The concepts were also covered in Lecture 8-9 if you want to review it. Later on you will have the chance to determine the order of the polynomial for the Z fit. You might see that we might not necessarily need a 6th order polynomial for the Z fit. The main reason for this is that we have much more data in W sample than the Z sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce7626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitW():\n",
    "    '''\n",
    "    You should define other functions in conjunction with a 6th order polynomial\n",
    "    '''\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd72abd2",
   "metadata": {},
   "source": [
    "2. After defining your model, you need to get the data histogram and perform the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1723929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we get the data histogram so we can fit it\n",
    "bins = 50\n",
    "mrange=[40,140]\n",
    "counts, bins = np.histogram(dataW,bins=bins,range=mrange,density=False)\n",
    "\n",
    "w = (1/ #poisson unc) #Poisson uncertainty here\n",
    "binCenters = (bins[1:]+bins[:-1])*.5\n",
    "x,y = binCenters.astype(\"float32\"), counts.astype(\"float32\")\n",
    "\n",
    "#Perform the fit \n",
    "model = lm.Model(fitW)\n",
    "p = model.make_params(#initial conditions for your fit) #You could experiment with zeros or your intuition first.\n",
    "\n",
    "#For better fit I suggest adding restrictions to the fit.\n",
    "\n",
    "result_W = model.fit(data=y,\n",
    "                   params=p,\n",
    "                   x=x,\n",
    "                   weights=w)\n",
    "\n",
    "#Plot the result\n",
    "plt.figure()\n",
    "result_W.plot()\n",
    "plt.xlabel(\"mass[GeV]\",position=(0.92,0.1))\n",
    "plt.ylabel(\"Entries/bin\",position=(0.1,0.84))\n",
    "\n",
    "#Print the fit summary\n",
    "print(result_W.fit_report())\n",
    "result_W.chisqr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5814c",
   "metadata": {},
   "source": [
    "Remember to compare your fit plot! Now we need to extract the W mass and the error in the measurement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93c22d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mW = result_W.params[\"mu\"].value\n",
    "mWerr = result_W.params[\"mu\"].stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea4d99e",
   "metadata": {},
   "source": [
    "Your **W mass should be in the 80-90 GeV** range depending on the fit function of your choice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68239a3a",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92307a0a",
   "metadata": {},
   "source": [
    "# 2. Fit for Z peak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85651d9",
   "metadata": {},
   "source": [
    "**NOTE: It is not mandatory that you completely finish this section before turning your work in!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86340aaa",
   "metadata": {},
   "source": [
    "Now that you have done the W fit, the next challenge is to see whether you can also perform the Z fit! They are very similar in concepts. However, the selection for Z requires a special cut: **the deep double-b tagging score (ddb).** This is because unlike the W, Z can decays into two b quarks and hence a cut on this score would remove many W signals. \n",
    "\n",
    "The codes to perform the fit should be very similar to what we have shown you in the last section! Your plots should come out similarly to this in the end:\n",
    "\n",
    "<img src=\"images/S50_ZFit.png\" width='900'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33158d3c",
   "metadata": {},
   "source": [
    "First, let's try to re-write the selection function. Remember that you now need to add the deep double b score of the jet and place a cut on it. The inividual b score in each subjet is already given in the `vjet0_sj1_csv` and `vjet0_sj2_csv` variable, how would you compute the b score of the whole jet?\n",
    "\n",
    "Another hint for placing the cut is this following table: \n",
    "\n",
    "<img src=\"images/S50_ZBtagCut.png\" width='500'>\n",
    "\n",
    "You could just ignore the labelling for the most part, the main point here is that there are three \"operating\" or \"working\" points: tight, medium, loose. Each according to specific cuts on the **individual** subjet b score. For example, a mistag rate of 0.1 would require the b score to be more than 0.9. The tight working point is also what is generally used. Using this information could you pick an appropriate threshold for the whole jet?\n",
    "\n",
    "An alternative approach to experimenting with different working points is for you to plot the distribution of the b scores of the Z signal versus W and QCD like we did in Section 0. From the histogram we could determine the appropriate cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8201a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a selection that will keep Z jets and remove background.\n",
    "def selectionZ(iData):\n",
    "    \n",
    "        #Standard preselection\n",
    "        trigger = (iData.arrays('trigger', library=\"np\")[\"trigger\"].flatten() >= 0)\n",
    "        \n",
    "        #Now lets require the jet pt to be above a threshold\n",
    "        jetpt   = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten() >= 400)\n",
    "        jetmass = (iData.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"].flatten())\n",
    "        jett2   = (iData.arrays('vjet0_t2', library=\"np\")[\"vjet0_t2\"].flatten())\n",
    "        jett1   = (iData.arrays('vjet0_t1', library=\"np\")[\"vjet0_t1\"].flatten())\n",
    "        jetptnocut = (iData.arrays('vjet0_pt', library=\"np\")[\"vjet0_pt\"].flatten())\n",
    "        jetcsvsj1  = (iData.arrays('vjet0_sj1_csv', library=\"np\")[\"vjet0_sj1_csv\"].flatten())\n",
    "        jetcsvsj2  = (iData.arrays('vjet0_sj2_csv', library=\"np\")[\"vjet0_sj2_csv\"].flatten())\n",
    "        \n",
    "        # Following the same procedure as https://arxiv.org/pdf/1603.00027.pdf\n",
    "        # we select 2 progned jets (since W and Z primarily decay to 2 quarks)\n",
    "        # we correct t21 to minimize the dependance on m and pt\n",
    "        rho = ### YOUR CODE HERE ###\n",
    "        t21 = ### YOUR CODE HERE ###\n",
    "        \n",
    "        t21ddt = ### YOUR CODE HERE ###\n",
    "        t21cut = ### YOUR CODE HERE ### #You could use a similar cut to W, but looser\n",
    "        \n",
    "        # this cut keeps jets with subjet 1 and subjet 2 coming from a secondary vertex\n",
    "        # i.e. the subjets originate from a displaced particle (namely a b quark)\n",
    "        # Unlike W bosons, the Z can decay into 2 b quarks which is why this selection removes \n",
    "        # many W jets.\n",
    "    \n",
    "        ddb = ### YOUR CODE HERE ###\n",
    "        ddbcut = ### YOUR CODE HERE ###\n",
    "        \n",
    "        allcuts = np.logical_and.reduce([trigger,jetpt, t21cut, ddbcut])\n",
    "        \n",
    "        return allcuts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f33c9e2",
   "metadata": {},
   "source": [
    "Similar to the W, let's try to make the mass plot first to see if we get the correct mass plot:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fca403",
   "metadata": {},
   "source": [
    "## 2.1 Z mass plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc1fd92",
   "metadata": {},
   "source": [
    "We can basically just reused what we had in the last section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6349c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some default parameters\n",
    "weights=[1000*18300,\"puweight\",\"scale1fb\"]\n",
    "mrange = (50,200) #range for mass histogram [GeV]\n",
    "bins=50           #bins for mass histogram\n",
    "density = True    #to plot the histograms as a density (integral=1)\n",
    "\n",
    "# Selecting data and MC \n",
    "qcdsel      = selectionZ(qcd)\n",
    "wsel        = selectionZ(wqq13)\n",
    "zsel        = selectionZ(zqq13)\n",
    "datasel     = selectionZ(data)\n",
    "ttsel       = selectionZ(tt)\n",
    "wwsel       = selectionZ(ww)\n",
    "wzsel       = selectionZ(wz)\n",
    "zzsel       = selectionZ(zz)\n",
    "gghsel      = selectionZ(ggh)\n",
    "\n",
    "wscale=scale(wqq,wqq13,weights)\n",
    "zscale=scale(zqq,zqq13,weights)\n",
    "\n",
    "# Getting the masses of selected events\n",
    "datat21 = data.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][datasel]\n",
    "qcdt21 = qcd.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][qcdsel]\n",
    "wt21 = wqq13.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][wsel]\n",
    "zt21 = zqq13.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][zsel]\n",
    "zzt21 = zz.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][zzsel]\n",
    "wzt21 = wz.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][wzsel]\n",
    "wwt21 = ww.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][wwsel]\n",
    "ttt21 = tt.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][ttsel]\n",
    "gght21 = ggh.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][gghsel]\n",
    "\n",
    "# Proper weights for each histogram\n",
    "hist_weights = [get_weights(qcd,weights,qcdsel),\n",
    "                get_weights(wqq13,weights,wsel)*wscale,\n",
    "                get_weights(zqq13,weights,zsel)*zscale,\n",
    "                get_weights(zz,weights,zzsel),\n",
    "                get_weights(wz,weights,wzsel),\n",
    "                get_weights(ww,weights,wwsel),\n",
    "                get_weights(tt,weights,ttsel),\n",
    "               ]\n",
    "\n",
    "# Plot stacked histograms\n",
    "plt.hist([qcdt21,wt21, zt21, zzt21, wzt21, wwt21, ttt21], \n",
    "         color=[\"royalblue\",\"r\", \"orange\",\"g\", \"b\", \"purple\", \"cyan\"], \n",
    "         label=[\"QCD\", \"W\", \"Z\", \"ZZ\", \"WZ\", \"WW\", \"tt\"], \n",
    "         weights=hist_weights,\n",
    "         range=mrange, bins=50, alpha=.6, density=density,stacked=True)\n",
    "counts, bins = np.histogram(datat21,bins=bins,range=mrange,density=density)\n",
    "\n",
    "# Getting the proper err requires normalizing since we are using a density not the actual count\n",
    "yerr = np.sqrt(counts) / np.sqrt(len(datat21)*(np.diff(bins))) \n",
    "binCenters = (bins[1:]+bins[:-1])*.5\n",
    "plt.errorbar(binCenters, counts, yerr=yerr,fmt=\"o\",c=\"k\",label=\"data\")\n",
    "plt.legend()\n",
    "plt.xlabel(r\"Mass [GeV]\")\n",
    "plt.ylabel(\"Normalized Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f9867",
   "metadata": {},
   "source": [
    "Make sure to compare your mass plot with the plot given above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc1466d",
   "metadata": {},
   "source": [
    "## 2.1 Fit for Z peak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e34c3be",
   "metadata": {},
   "source": [
    "Now, adapting the codes provided for you in the W section, perform a fit to the Z peak and extract its mass! The concept is similar to our previous W fit, but this time I would let you explore what fit function would be appropriate in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitZ(x,):\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff7da5",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a474313",
   "metadata": {},
   "source": [
    "Then, get the data histogram and perform the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we get the data histogram so we can fit it\n",
    "counts, bins = np.histogram(datat21,\n",
    "                            bins=40,\n",
    "                            range=mrange)\n",
    "\n",
    "w = 1/np.sqrt(counts)\n",
    "binCenters = (bins[1:]+bins[:-1])*.5\n",
    "x,y = binCenters.astype(\"float32\"), counts.astype(\"float32\")\n",
    "\n",
    "#Perform the fit\n",
    "model = lm.Model(fitZ)\n",
    "p = model.make_params(p0=10,\n",
    "                      p1=50,\n",
    "                      p2=0,\n",
    "                      p3=50,\n",
    "                      p4=0,\n",
    "                      p5=0,\n",
    "                      a=0,\n",
    "                      mu=90,\n",
    "                      sigma=5)\n",
    "\n",
    "result_Z = model.fit(data=y,\n",
    "                     params=p,\n",
    "                     x=x,\n",
    "                     weights=w)\n",
    "\n",
    "#Plottting\n",
    "plt.figure()\n",
    "result_Z.plot()\n",
    "plt.xlabel(\"mass[GeV]\", position=(0.92,0.1))\n",
    "plt.ylabel(\"Entries/bin\", position=(0.1,0.84))\n",
    "\n",
    "#We could also check out all the fit parameters\n",
    "print(result_Z.fit_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca9f65",
   "metadata": {},
   "source": [
    "After the fit we can extract the Z mass and the error in our measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mZ = result_Z.params[\"mu\"].value\n",
    "mZerr = result_Z.params[\"mu\"].stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a82d915",
   "metadata": {},
   "source": [
    "Note that the Z mass should be around **91 GeV**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8da829b",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb9f883",
   "metadata": {},
   "source": [
    "# 3. Weak mixing angle (OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca4a6c8",
   "metadata": {},
   "source": [
    "If you have both W and Z boson, you can measure a fundamental parameter of the standard model. The Weak mixing angle, sometimes referred to as the Weinberg angle. \n",
    "\n",
    "<font color=\"blue\"> From your measurement of the W and Z boson mass come up with a measurement of the W and Z boson mixing angle.  </font>\n",
    "\n",
    "we have that $\\sin^2\\theta_w = 1 - (\\frac{m_W}{m_Z})^2$ which was measured to be about 0.231. Using the masses we measured and propagating the error gives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55569d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin2theta = 1 - (mW/mZ)**2\n",
    "sin2theta_err = ### YOUR CODE HERE ###\n",
    "\n",
    "print(f\"sin^2(θ) = {sin2theta:.3f} +\\- {sin2theta_err:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba47109a",
   "metadata": {},
   "source": [
    "Hmm... It looks like our measurement is pretty bad.  The reason for this is that we overestimated the mass of the W boson because many of the \"signal\" events selected contain a Z boson which has higher mass. Because we can't easily find a selection that removes Z jets and keeps W jets we will resort to making a correction from the Monte Carlo (MC). Since we have control over what samples go into the MC we can actually do our fit with and without Z jets to see how biased our procedure is. Then we can correct for this bias approriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f8cf9d",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b941a",
   "metadata": {},
   "source": [
    "# 4. Correcting $m_W$   ( OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd6aa1",
   "metadata": {},
   "source": [
    "First we need to perform the selection for W again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b33f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Look at the mass\n",
    "weights = [1000*18300, \"puweight\", \"scale1fb\"]\n",
    "mrange = (45,200)  #range for mass histogram [GeV]\n",
    "bins=40            #bins for mass histogram\n",
    "density = True     #to plot the histograms as a density (integral=1)\n",
    "\n",
    "qcdsel      = selectionW(qcd)\n",
    "wsel        = selectionW(wqq13)\n",
    "zsel        = selectionW(zqq13)\n",
    "datasel     = selectionW(data)\n",
    "ttsel       = selectionW(tt)\n",
    "wwsel       = selectionW(ww)\n",
    "wzsel       = selectionW(wz)\n",
    "zzsel       = selectionW(zz)\n",
    "gghsel      = selectionW(ggh)\n",
    "wscale=scale(wqq,wqq13,weights)\n",
    "zscale=scale(zqq,zqq13,weights)\n",
    "\n",
    "dataW = data.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][datasel]\n",
    "qcdW = qcd.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][qcdsel]\n",
    "wW = wqq13.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][wsel]\n",
    "zW = zqq13.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][zsel]\n",
    "zzW = zz.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][zzsel]\n",
    "wzW = wz.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][wzsel]\n",
    "wwW = ww.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][wwsel]\n",
    "ttW = tt.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][ttsel]\n",
    "gghW = ggh.arrays('vjet0_msd0', library=\"np\")[\"vjet0_msd0\"][gghsel]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded5ec57",
   "metadata": {},
   "source": [
    "After that we could evaluate the MC simulation in two cases:\n",
    "\n",
    "1. MC with Z\n",
    "2. MC without Z\n",
    "\n",
    "And then based on the fit of these two samples, we could evaluate the ratio of the fitted W mass and determine the bias factor, which can then be used to correct the W mass!\n",
    "\n",
    "In this lab we can perform a simple procedure to determine the bias by plotting the MC distribution of the W mass with selection. After that, we could take the mean and standard deviation with and without the Z to feed into our bias calculation! A more advanced approach would be to perform the whole W fit again with QCD MC. However, the fit is rather hard to control (getting a good $\\chi^2$ is hard). Therefore, for the purposes of this course, we would resort to a simpler verion first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d9cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins=50\n",
    "\n",
    "# We need the weights from our MC\n",
    "wweights = get_weights(wqq13,weights,wsel)*wscale\n",
    "zweights = get_weights(zqq13,weights,zsel)*zscale\n",
    "\n",
    "mrange = [50,150]\n",
    "\n",
    "# Now we stack the histograms into a single density \n",
    "counts_mc, bins,_ = plt.hist([wW,zW],bins=bins,range=mrange,\n",
    "                             density=False,stacked=True,\n",
    "                             color=[\"r\", \"orange\"],\n",
    "                             alpha=.6,\n",
    "                             weights= [wweights, zweights])\n",
    "\n",
    "\n",
    "binCenters = (bins[1:]+bins[:-1])*.5\n",
    "plt.show()\n",
    "\n",
    "mW_mc_withZ = np.average(binCenters, weights=counts_mc[-1])\n",
    "mWerr_mc_withZ = ### YOUR CODE HERE ###\n",
    "\n",
    "mW_mc_noZ = np.average(binCenters,weights=counts_mc[0])\n",
    "mWerr_mc_noZ = ### YOUR CODE HERE ###\n",
    "\n",
    "print(\"W mass with Z: {}. W mass without Z: {}\".format(mW_mc_withZ, mW_mc_noZ))\n",
    "print(\"W mass (standard deviation) with Z: {}. W mass (standard deviation) without Z: {}\".format(mWerr_mc_withZ, mWerr_mc_noZ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a11ba",
   "metadata": {},
   "source": [
    "Then we can obtain the mass with no Z in the sample:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef47db4",
   "metadata": {},
   "source": [
    "## 4.3 Bias factor and correct for W mass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aadf0d",
   "metadata": {},
   "source": [
    "Now that we obtain the W mass in both cases, let's correct for the W mass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df87044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we calculate our bias factor\n",
    "# if the W mass measured with and without Z is the same \n",
    "# this factor should be 1\n",
    "bias = mW_mc_withZ / mW_mc_noZ\n",
    "\n",
    "#propagate the error assuming uncorrelated gaussian uncertainties\n",
    "bias_err = ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6fbd7",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mW_corrected = mW/bias\n",
    "mWerr_corrected = ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb06ef",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f3305",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"Correcting the W mass using MC (simulated data) gives m_W = {mW_corrected:.1f} +\\- {mWerr_corrected:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c8bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin2theta = 1 - (mW_corrected/mZ)**2\n",
    "sin2theta_err = ### YOUR CODE HERE ###\n",
    "print(f\"sin^2(θ) = {sin2theta:.3f} +\\- {sin2theta_err:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08614cdf",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70616e97",
   "metadata": {},
   "source": [
    "And the new $Sin^2(\\theta)$ should be at around:\n",
    "\n",
    "$$Sin^2(\\theta) = 0.21$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64461b1d",
   "metadata": {},
   "source": [
    "This measurement is much better than the previous one! Congratulations on making it this far to this lab!\n",
    "\n",
    "Now if you have made it through this whole exercise, why don't you try to look for the Higgs boson. The Higgs boson also decays hadronically and decays predominantly to a certainy pair of particles. Take a look [here](http://pdg.lbl.gov/2019/reviews/rpp2019-rev-higgs-boson.pdf). \n",
    " <font color=\"blue\"> Can you search for the Higgs boson? What is your sensitivity.  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce51a831",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17ce76",
   "metadata": {},
   "source": [
    "## 4. Search for the Higgs Boson! (OPTIONAL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
